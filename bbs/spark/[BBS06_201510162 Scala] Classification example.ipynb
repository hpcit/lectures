{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Decision Tree and Naive Bayes example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MLlib libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.configuration.Algo\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Iris flower data set** or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\". The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rawData = sc.textFile(\"data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5.1, 3.5, 1.4, 0.2, Iris-setosa)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val splitlines = rawData.map(lines => {\n",
    "    lines.split(',')\n",
    "  })\n",
    "splitlines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[5.1,3.5,1.4,0.2])\n",
      "(1.0,[4.9,3.0,1.4,0.2])\n",
      "(1.0,[4.7,3.2,1.3,0.2])\n",
      "(1.0,[4.6,3.1,1.5,0.2])\n",
      "(1.0,[5.0,3.6,1.4,0.2])\n"
     ]
    }
   ],
   "source": [
    "val Data = splitlines.map { col =>   \n",
    "     val species = col(col.size - 1)                       \n",
    "     val label = if (species == \"Iris-versicolor\") 0.toInt else if (species == \"Iris-setosa\") 1.toInt else 2.toInt\n",
    "     val features = col.slice(0, col.size - 1).map(_.toDouble)\n",
    "     LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "Data.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets (40% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "(1.0,[5.1,3.5,1.4,0.2])\n",
      "(1.0,[4.9,3.0,1.4,0.2])\n",
      "(1.0,[4.7,3.2,1.3,0.2])\n",
      "(1.0,[4.6,3.1,1.5,0.2])\n",
      "(1.0,[5.0,3.6,1.4,0.2])\n",
      "(1.0,[5.4,3.9,1.7,0.4])\n",
      "(1.0,[4.6,3.4,1.4,0.3])\n",
      "(1.0,[5.0,3.4,1.5,0.2])\n",
      "(1.0,[4.4,2.9,1.4,0.2])\n",
      "(1.0,[4.8,3.4,1.6,0.2])\n",
      "(1.0,[4.8,3.0,1.4,0.1])\n",
      "(1.0,[4.3,3.0,1.1,0.1])\n",
      "(1.0,[5.7,4.4,1.5,0.4])\n",
      "(1.0,[5.1,3.5,1.4,0.3])\n",
      "(1.0,[5.7,3.8,1.7,0.3])\n",
      "(1.0,[5.4,3.4,1.7,0.2])\n",
      "(1.0,[5.1,3.3,1.7,0.5])\n",
      "(1.0,[5.0,3.0,1.6,0.2])\n",
      "(1.0,[5.2,3.5,1.5,0.2])\n",
      "(1.0,[5.2,3.4,1.4,0.2])\n",
      "(1.0,[4.8,3.1,1.6,0.2])\n",
      "(1.0,[5.5,4.2,1.4,0.2])\n",
      "(1.0,[5.0,3.2,1.2,0.2])\n",
      "(1.0,[5.5,3.5,1.3,0.2])\n",
      "(1.0,[4.9,3.6,1.4,0.1])\n",
      "(1.0,[5.1,3.4,1.5,0.2])\n",
      "(1.0,[4.8,3.0,1.4,0.3])\n",
      "(1.0,[5.1,3.8,1.6,0.2])\n",
      "(1.0,[4.6,3.2,1.4,0.2])\n",
      "(1.0,[5.3,3.7,1.5,0.2])\n",
      "(0.0,[6.4,3.2,4.5,1.5])\n",
      "(0.0,[6.9,3.1,4.9,1.5])\n",
      "(0.0,[5.5,2.3,4.0,1.3])\n",
      "(0.0,[5.7,2.8,4.5,1.3])\n",
      "(0.0,[6.3,3.3,4.7,1.6])\n",
      "(0.0,[4.9,2.4,3.3,1.0])\n",
      "(0.0,[5.2,2.7,3.9,1.4])\n",
      "(0.0,[5.0,2.0,3.5,1.0])\n",
      "(0.0,[5.9,3.0,4.2,1.5])\n",
      "(0.0,[6.0,2.2,4.0,1.0])\n",
      "(0.0,[6.1,2.9,4.7,1.4])\n",
      "(0.0,[5.6,3.0,4.5,1.5])\n",
      "(0.0,[5.8,2.7,4.1,1.0])\n",
      "(0.0,[5.6,2.5,3.9,1.1])\n",
      "(0.0,[5.9,3.2,4.8,1.8])\n",
      "(0.0,[6.1,2.8,4.7,1.2])\n",
      "(0.0,[6.4,2.9,4.3,1.3])\n",
      "(0.0,[6.6,3.0,4.4,1.4])\n",
      "(0.0,[6.8,2.8,4.8,1.4])\n",
      "(0.0,[6.7,3.0,5.0,1.7])\n",
      "(0.0,[6.0,2.9,4.5,1.5])\n",
      "(0.0,[5.5,2.4,3.7,1.0])\n",
      "(0.0,[5.8,2.7,3.9,1.2])\n",
      "(0.0,[6.0,2.7,5.1,1.6])\n",
      "(0.0,[5.4,3.0,4.5,1.5])\n",
      "(0.0,[6.0,3.4,4.5,1.6])\n",
      "(0.0,[6.7,3.1,4.7,1.5])\n",
      "(0.0,[6.3,2.3,4.4,1.3])\n",
      "(0.0,[5.6,3.0,4.1,1.3])\n",
      "(0.0,[5.5,2.5,4.0,1.3])\n",
      "(0.0,[5.5,2.6,4.4,1.2])\n",
      "(0.0,[6.1,3.0,4.6,1.4])\n",
      "(0.0,[5.6,2.7,4.2,1.3])\n",
      "(0.0,[5.1,2.5,3.0,1.1])\n",
      "(0.0,[5.7,2.8,4.1,1.3])\n",
      "(2.0,[6.3,3.3,6.0,2.5])\n",
      "(2.0,[5.8,2.7,5.1,1.9])\n",
      "(2.0,[6.5,3.0,5.8,2.2])\n",
      "(2.0,[7.6,3.0,6.6,2.1])\n",
      "(2.0,[4.9,2.5,4.5,1.7])\n",
      "(2.0,[6.5,3.2,5.1,2.0])\n",
      "(2.0,[6.4,2.7,5.3,1.9])\n",
      "(2.0,[5.7,2.5,5.0,2.0])\n",
      "(2.0,[6.4,3.2,5.3,2.3])\n",
      "(2.0,[6.5,3.0,5.5,1.8])\n",
      "(2.0,[7.7,3.8,6.7,2.2])\n",
      "(2.0,[7.7,2.6,6.9,2.3])\n",
      "(2.0,[6.9,3.2,5.7,2.3])\n",
      "(2.0,[5.6,2.8,4.9,2.0])\n",
      "(2.0,[7.7,2.8,6.7,2.0])\n",
      "(2.0,[6.3,2.7,4.9,1.8])\n",
      "(2.0,[6.7,3.3,5.7,2.1])\n",
      "(2.0,[7.2,3.2,6.0,1.8])\n",
      "(2.0,[6.2,2.8,4.8,1.8])\n",
      "(2.0,[6.4,2.8,5.6,2.1])\n",
      "(2.0,[7.4,2.8,6.1,1.9])\n",
      "(2.0,[7.9,3.8,6.4,2.0])\n",
      "(2.0,[6.4,2.8,5.6,2.2])\n",
      "(2.0,[6.3,2.8,5.1,1.5])\n",
      "(2.0,[6.1,2.6,5.6,1.4])\n",
      "(2.0,[6.3,3.4,5.6,2.4])\n",
      "(2.0,[6.4,3.1,5.5,1.8])\n",
      "(2.0,[6.0,3.0,4.8,1.8])\n",
      "(2.0,[6.7,3.1,5.6,2.4])\n",
      "(2.0,[6.9,3.1,5.1,2.3])\n",
      "(2.0,[5.8,2.7,5.1,1.9])\n",
      "(2.0,[6.8,3.2,5.9,2.3])\n",
      "(2.0,[6.7,3.3,5.7,2.5])\n",
      "(2.0,[6.7,3.0,5.2,2.3])\n",
      "(2.0,[6.5,3.0,5.2,2.0])\n",
      "Test Data\n",
      "(1.0,[4.9,3.1,1.5,0.1])\n",
      "(1.0,[5.4,3.7,1.5,0.2])\n",
      "(1.0,[5.8,4.0,1.2,0.2])\n",
      "(1.0,[5.4,3.9,1.3,0.4])\n",
      "(1.0,[5.1,3.8,1.5,0.3])\n",
      "(1.0,[5.1,3.7,1.5,0.4])\n",
      "(1.0,[4.6,3.6,1.0,0.2])\n",
      "(1.0,[4.8,3.4,1.9,0.2])\n",
      "(1.0,[5.0,3.4,1.6,0.4])\n",
      "(1.0,[4.7,3.2,1.6,0.2])\n",
      "(1.0,[5.4,3.4,1.5,0.4])\n",
      "(1.0,[5.2,4.1,1.5,0.1])\n",
      "(1.0,[4.9,3.1,1.5,0.2])\n",
      "(1.0,[4.4,3.0,1.3,0.2])\n",
      "(1.0,[5.0,3.5,1.3,0.3])\n",
      "(1.0,[4.5,2.3,1.3,0.3])\n",
      "(1.0,[4.4,3.2,1.3,0.2])\n",
      "(1.0,[5.0,3.5,1.6,0.6])\n",
      "(1.0,[5.1,3.8,1.9,0.4])\n",
      "(1.0,[5.0,3.3,1.4,0.2])\n",
      "(0.0,[7.0,3.2,4.7,1.4])\n",
      "(0.0,[6.5,2.8,4.6,1.5])\n",
      "(0.0,[6.6,2.9,4.6,1.3])\n",
      "(0.0,[5.6,2.9,3.6,1.3])\n",
      "(0.0,[6.7,3.1,4.4,1.4])\n",
      "(0.0,[6.2,2.2,4.5,1.5])\n",
      "(0.0,[6.1,2.8,4.0,1.3])\n",
      "(0.0,[6.3,2.5,4.9,1.5])\n",
      "(0.0,[5.7,2.6,3.5,1.0])\n",
      "(0.0,[5.5,2.4,3.8,1.1])\n",
      "(0.0,[5.8,2.6,4.0,1.2])\n",
      "(0.0,[5.0,2.3,3.3,1.0])\n",
      "(0.0,[5.7,3.0,4.2,1.2])\n",
      "(0.0,[5.7,2.9,4.2,1.3])\n",
      "(0.0,[6.2,2.9,4.3,1.3])\n",
      "(2.0,[7.1,3.0,5.9,2.1])\n",
      "(2.0,[6.3,2.9,5.6,1.8])\n",
      "(2.0,[7.3,2.9,6.3,1.8])\n",
      "(2.0,[6.7,2.5,5.8,1.8])\n",
      "(2.0,[7.2,3.6,6.1,2.5])\n",
      "(2.0,[6.8,3.0,5.5,2.1])\n",
      "(2.0,[5.8,2.8,5.1,2.4])\n",
      "(2.0,[6.0,2.2,5.0,1.5])\n",
      "(2.0,[6.1,3.0,4.9,1.8])\n",
      "(2.0,[7.2,3.0,5.8,1.6])\n",
      "(2.0,[7.7,3.0,6.1,2.3])\n",
      "(2.0,[6.9,3.1,5.4,2.1])\n",
      "(2.0,[6.3,2.5,5.0,1.9])\n",
      "(2.0,[6.2,3.4,5.4,2.3])\n",
      "(2.0,[5.9,3.0,5.1,1.8])\n"
     ]
    }
   ],
   "source": [
    "val splits = Data.randomSplit(Array(0.6, 0.4), seed = 11L)\n",
    "val trainingData = splits(0).cache()\n",
    "val testData = splits(1)\n",
    "println(\"Training Data\")\n",
    "trainingData.take(100).foreach(println)\n",
    "println(\"Test Data\")\n",
    "testData.take(100).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling and are able to capture nonlinearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.\n",
    "MLlib supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 9 nodes\n",
      "  If (feature 2 <= 1.7)\n",
      "   Predict: 1.0\n",
      "  Else (feature 2 > 1.7)\n",
      "   If (feature 3 <= 1.7)\n",
      "    If (feature 2 <= 5.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 > 5.0)\n",
      "     Predict: 2.0\n",
      "   Else (feature 3 > 1.7)\n",
      "    If (feature 0 <= 6.0)\n",
      "     Predict: 2.0\n",
      "    Else (feature 0 > 6.0)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val numClasses = 3\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"entropy\"\n",
    "val maxDepth = 3\n",
    "val maxBins = 10\n",
    "val dtModel = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  impurity, maxDepth, maxBins)\n",
    "println(dtModel.toDebugString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "val dtTotalCorrect = trainingData.map { point =>\n",
    "  if (dtModel.predict(point.features) == point.label) 1 else 0\n",
    "  }.sum\n",
    "\n",
    "println(dtTotalCorrect)\n",
    "println(trainingData.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "val dtAccuracy = dtTotalCorrect / trainingData.count\n",
    "println(dtAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "val dtTotalCorrect = testData.map { point =>\n",
    "  if (dtModel.predict(point.features) == point.label) 1 else 0\n",
    "  }.sum\n",
    "println(dtTotalCorrect)\n",
    "println(testData.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n"
     ]
    }
   ],
   "source": [
    "val dtAccuracy = dtTotalCorrect / testData.count\n",
    "println(dtAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a simple multiclass classification algorithm with the assumption of independence between every pair of features. Naive Bayes can be trained very efficiently. Within a single pass to the training data, it computes the conditional probability distribution of each feature given label, and then it applies Bayes theorem to compute the conditional probability distribution of label given an observation and use it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.mllib.classification.NaiveBayesModel@792f5b63\n"
     ]
    }
   ],
   "source": [
    "val nbModel = NaiveBayes.train(trainingData)\n",
    "println(nbModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "val nbTotalCorrect = trainingData.map { point =>\n",
    "    if (nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "println(nbTotalCorrect)\n",
    "println(trainingData.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "val nbAccuracy = nbTotalCorrect / trainingData.count\n",
    "println(nbAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.0\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "val nbTotalCorrect = testData.map { point =>\n",
    "    if (nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "println(nbTotalCorrect)\n",
    "println(testData.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "val nbAccuracy = nbTotalCorrect / testData.count\n",
    "println(nbAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Complete evaluation on test set for Decision Tree  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "15.0  0.0   0.0   \n",
      "2.0   18.0  0.0   \n",
      "1.0   0.0   14.0  \n",
      "Summary Statistics\n",
      "Precision = 0.94\n",
      "Recall = 0.94\n",
      "F1 Score = 0.94\n",
      "Precision(0.0) = 0.8333333333333334\n",
      "Precision(1.0) = 1.0\n",
      "Precision(2.0) = 1.0\n",
      "Recall(0.0) = 1.0\n",
      "Recall(1.0) = 0.9\n",
      "Recall(2.0) = 0.9333333333333333\n",
      "FPR(0.0) = 0.08571428571428572\n",
      "FPR(1.0) = 0.0\n",
      "FPR(2.0) = 0.0\n",
      "F1-Score(0.0) = 0.9090909090909091\n",
      "F1-Score(1.0) = 0.9473684210526316\n",
      "F1-Score(2.0) = 0.9655172413793104\n",
      "Weighted precision: 0.9500000000000001\n",
      "Weighted recall: 0.9400000000000001\n",
      "Weighted F1 score: 0.9413298135621185\n",
      "Weighted false positive rate: 0.025714285714285717\n"
     ]
    }
   ],
   "source": [
    "val predictionAndLabels = testData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = dtModel.predict(features)\n",
    "  (prediction, label)\n",
    "}\n",
    "\n",
    "// Instantiate metrics object\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)\n",
    "\n",
    "// Overall Statistics\n",
    "val precision = metrics.precision\n",
    "val recall = metrics.recall // same as true positive rate\n",
    "val f1Score = metrics.fMeasure\n",
    "println(\"Summary Statistics\")\n",
    "println(s\"Precision = $precision\")\n",
    "println(s\"Recall = $recall\")\n",
    "println(s\"F1 Score = $f1Score\")\n",
    "\n",
    "// Precision by label\n",
    "val labels = metrics.labels\n",
    "labels.foreach { l =>\n",
    "    println(s\"Precision($l) = \" + metrics.precision(l))\n",
    "}\n",
    "\n",
    "// Recall by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"Recall($l) = \" + metrics.recall(l))\n",
    "}\n",
    "\n",
    "// False positive rate by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n",
    "}\n",
    "\n",
    "// F-measure by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n",
    "}\n",
    "\n",
    "// Weighted stats\n",
    "println(s\"Weighted precision: ${metrics.weightedPrecision}\")\n",
    "println(s\"Weighted recall: ${metrics.weightedRecall}\")\n",
    "println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n",
    "println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete evaluation on test set for Naive Bayes  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "14.0  0.0   1.0   \n",
      "0.0   20.0  0.0   \n",
      "1.0   0.0   14.0  \n",
      "Summary Statistics\n",
      "Precision = 0.96\n",
      "Recall = 0.96\n",
      "F1 Score = 0.96\n",
      "Precision(0.0) = 0.9333333333333333\n",
      "Precision(1.0) = 1.0\n",
      "Precision(2.0) = 0.9333333333333333\n",
      "Recall(0.0) = 0.9333333333333333\n",
      "Recall(1.0) = 1.0\n",
      "Recall(2.0) = 0.9333333333333333\n",
      "FPR(0.0) = 0.02857142857142857\n",
      "FPR(1.0) = 0.0\n",
      "FPR(2.0) = 0.02857142857142857\n",
      "F1-Score(0.0) = 0.9333333333333333\n",
      "F1-Score(1.0) = 1.0\n",
      "F1-Score(2.0) = 0.9333333333333333\n",
      "Weighted precision: 0.9600000000000001\n",
      "Weighted recall: 0.9600000000000001\n",
      "Weighted F1 score: 0.9600000000000001\n",
      "Weighted false positive rate: 0.01714285714285714\n"
     ]
    }
   ],
   "source": [
    "val predictionAndLabels = testData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = nbModel.predict(features)\n",
    "  (prediction, label)\n",
    "}\n",
    "\n",
    "// Instantiate metrics object\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)\n",
    "\n",
    "// Overall Statistics\n",
    "val precision = metrics.precision\n",
    "val recall = metrics.recall // same as true positive rate\n",
    "val f1Score = metrics.fMeasure\n",
    "println(\"Summary Statistics\")\n",
    "println(s\"Precision = $precision\")\n",
    "println(s\"Recall = $recall\")\n",
    "println(s\"F1 Score = $f1Score\")\n",
    "\n",
    "// Precision by label\n",
    "val labels = metrics.labels\n",
    "labels.foreach { l =>\n",
    "    println(s\"Precision($l) = \" + metrics.precision(l))\n",
    "}\n",
    "\n",
    "// Recall by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"Recall($l) = \" + metrics.recall(l))\n",
    "}\n",
    "\n",
    "// False positive rate by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n",
    "}\n",
    "\n",
    "// F-measure by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n",
    "}\n",
    "\n",
    "// Weighted stats\n",
    "println(s\"Weighted precision: ${metrics.weightedPrecision}\")\n",
    "println(s\"Weighted recall: ${metrics.weightedRecall}\")\n",
    "println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n",
    "println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
