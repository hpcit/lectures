{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Decision Tree and Naive Bayes example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MLlib libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "import pyspark.mllib.linalg\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and pre-processing\n",
    "The **Iris flower data set** or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\". The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawData = sc.textFile(\"data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.1', '3.5', '1.4', '0.2', 'Iris-setosa']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "val splitlines = rawData.map(lines => {\n",
    "    lines.split(',')\n",
    "  })\n",
    "splitlines.first()\n",
    "'''\n",
    "splitlines = rawData.map(lambda lines: lines.split(','))\n",
    "splitlines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[5.1,3.5,1.4,0.2])\n",
      "(1.0,[4.9,3.0,1.4,0.2])\n",
      "(1.0,[4.7,3.2,1.3,0.2])\n",
      "(1.0,[4.6,3.1,1.5,0.2])\n",
      "(1.0,[5.0,3.6,1.4,0.2])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val Data = splitlines.map { col =>   \n",
    "     val species = col(col.size - 1)                       \n",
    "     val label = if (species == \"Iris-versicolor\") 0.toInt else if (species == \"Iris-setosa\") 1.toInt else 2.toInt\n",
    "     val features = col.slice(0, col.size - 1).map(_.toDouble)\n",
    "     LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "Data.take(5).foreach(println)\n",
    "'''\n",
    "def iris_label(argument):\n",
    "    iris = {\n",
    "        \"Iris-versicolor\": 0,\n",
    "        \"Iris-setosa\": 1,\n",
    "        \"Iris-virginica\": 2,\n",
    "    }\n",
    "    return iris.get(argument, 0)\n",
    "\n",
    "def estractor(col):\n",
    "    species = col[-1]\n",
    "    label = iris_label(species)\n",
    "    features = [float(x) for x in col[:-1]]\n",
    "    return LabeledPoint(label, Vectors.dense(features))\n",
    "    \n",
    "Data = splitlines.map(estractor)\n",
    "\n",
    "for x in Data.take(5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets (40% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "Training Data\n",
      "(1.0,[5.1,3.5,1.4,0.2])\n",
      "(1.0,[4.9,3.0,1.4,0.2])\n",
      "(1.0,[5.0,3.6,1.4,0.2])\n",
      "(1.0,[5.4,3.9,1.7,0.4])\n",
      "(1.0,[4.6,3.4,1.4,0.3])\n",
      "Test Data\n",
      "(1.0,[4.7,3.2,1.3,0.2])\n",
      "(1.0,[4.6,3.1,1.5,0.2])\n",
      "(1.0,[5.0,3.4,1.5,0.2])\n",
      "(1.0,[4.4,2.9,1.4,0.2])\n",
      "(1.0,[4.9,3.1,1.5,0.1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val splits = Data.randomSplit(Array(0.6, 0.4), seed = 11L)\n",
    "val trainingData = splits(0).cache()\n",
    "val testData = splits(1)\n",
    "println(\"Training Data\")\n",
    "trainingData.take(5).foreach(println)\n",
    "println(\"Test Data\")\n",
    "testData.take(5).foreach(println)\n",
    "'''\n",
    "splits = Data.randomSplit([0.6, 0.4], seed = 11)\n",
    "trainingData = splits[0].cache()\n",
    "testData = splits[1]\n",
    "print(trainingData.count())\n",
    "print(\"Training Data\")\n",
    "for x in trainingData.take(5):\n",
    "    print(x)\n",
    "print(\"Test Data\")\n",
    "for x in testData.take(5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Decision Tree\n",
    "Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling and are able to capture nonlinearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks. MLlib supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 13 nodes\n",
      "  If (feature 2 <= 3.5)\n",
      "   If (feature 1 <= 2.6)\n",
      "    If (feature 0 <= 4.8)\n",
      "     Predict: 1.0\n",
      "    Else (feature 0 > 4.8)\n",
      "     Predict: 0.0\n",
      "   Else (feature 1 > 2.6)\n",
      "    Predict: 1.0\n",
      "  Else (feature 2 > 3.5)\n",
      "   If (feature 3 <= 1.7)\n",
      "    If (feature 2 <= 5.3)\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 > 5.3)\n",
      "     Predict: 2.0\n",
      "   Else (feature 3 > 1.7)\n",
      "    If (feature 2 <= 5.0)\n",
      "     Predict: 2.0\n",
      "    Else (feature 2 > 5.0)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val numClasses = 3\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"entropy\"\n",
    "val maxDepth = 3\n",
    "val maxBins = 10\n",
    "val dtModel = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  impurity, maxDepth, maxBins)\n",
    "println(dtModel.toDebugString)\n",
    "'''\n",
    "numClasses = 3\n",
    "categoricalFeaturesInfo = {} #Map[Int, Int]()\n",
    "impurity = \"entropy\"\n",
    "maxDepth = 3\n",
    "maxBins = 10\n",
    "dtModel = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)\n",
    "print(dtModel.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val dtTotalCorrect = trainingData.map { point =>\n",
    "  if (dtModel.predict(point.features) == point.label) 1 else 0\n",
    "  }.sum\n",
    "\n",
    "println(dtTotalCorrect)\n",
    "println(trainingData.count)\n",
    "'''\n",
    "predictions = dtModel.predict(trainingData.map(lambda point: point.features))\n",
    "labelsAndPredictions = trainingData.map(lambda lp: lp.label).zip(predictions)\n",
    "dtTotalCorrect = labelsAndPredictions.map(lambda line: 1 if line[0]==line[1] else 0).sum()\n",
    "\n",
    "'''\n",
    "for x in trainingData.collect():\n",
    "    print(dtModel.predict(x.features))\n",
    "'''\n",
    "\n",
    "print(dtTotalCorrect)\n",
    "print(trainingData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9693877551020408\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val dtAccuracy = dtTotalCorrect / trainingData.count\n",
    "println(dtAccuracy)\n",
    "'''\n",
    "dtAccuracy = dtTotalCorrect / trainingData.count()\n",
    "print(dtAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val dtTotalCorrect = testData.map { point =>\n",
    "    if (nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "println(dtTotalCorrect)\n",
    "println(testData.count)\n",
    "'''\n",
    "predictions = dtModel.predict(testData.map(lambda point: point.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "dtTotalCorrect = labelsAndPredictions.map(lambda line: 1 if line[0]==line[1] else 0).sum()\n",
    "\n",
    "print(dtTotalCorrect)\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9807692307692307\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val nbAccuracy = nbTotalCorrect / testData.count\n",
    "println(nbAccuracy)\n",
    "'''\n",
    "dtAccuracy = dtTotalCorrect / testData.count()\n",
    "print(dtAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Naive Bayes\n",
    "Naive Bayes is a simple multiclass classification algorithm with the assumption of independence between every pair of features. Naive Bayes can be trained very efficiently. Within a single pass to the training data, it computes the conditional probability distribution of each feature given label, and then it applies Bayes theorem to compute the conditional probability distribution of label given an observation and use it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.mllib.classification.NaiveBayesModel object at 0x7f3d8bd6e780>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val nbModel = NaiveBayes.train(trainingData)\n",
    "println(nbModel)\n",
    "'''\n",
    "nbModel = NaiveBayes.train(trainingData,1.0)\n",
    "print(nbModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val nbTotalCorrect = trainingData.map { point =>\n",
    "    if (nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "println(nbTotalCorrect)\n",
    "println(trainingData.count)\n",
    "'''\n",
    "predictions = nbModel.predict(trainingData.map(lambda point: point.features))\n",
    "labelsAndPredictions = trainingData.map(lambda lp: lp.label).zip(predictions)\n",
    "nbTotalCorrect = labelsAndPredictions.map(lambda line: 1 if line[0]==line[1] else 0).sum()\n",
    "print(nbTotalCorrect)\n",
    "print(trainingData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val nbAccuracy = nbTotalCorrect / trainingData.count\n",
    "println(nbAccuracy)\n",
    "'''\n",
    "nbAccuracy = nbTotalCorrect / trainingData.count()\n",
    "print(nbAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val nbTotalCorrect = testData.map { point =>\n",
    "    if (nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "println(nbTotalCorrect)\n",
    "println(testData.count)\n",
    "'''\n",
    "predictions = nbModel.predict(testData.map(lambda point: point.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "nbTotalCorrect = labelsAndPredictions.map(lambda line: 1 if line[0]==line[1] else 0).sum()\n",
    "print(nbTotalCorrect)\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val nbAccuracy = nbTotalCorrect / testData.count\n",
    "println(nbAccuracy)\n",
    "'''\n",
    "nbAccuracy = nbTotalCorrect / testData.count()\n",
    "print(nbAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete evaluation on test set for Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Summary Statistics\n",
      "Precision =  0.9807692307692307\n",
      "Recall =  0.9807692307692307\n",
      "F1 Score =  0.9807692307692307\n",
      "Precision(0.0) = 0.9565217391304348\n",
      "Precision(1.0) = 1.0\n",
      "Precision(2.0) = 1.0\n",
      "Recall(0.0) = 1.0\n",
      "Recall(1.0) = 1.0\n",
      "Recall(2.0) = 0.9333333333333333\n",
      "FPR(0.0) = 0.03333333333333333\n",
      "FPR(1.0) = 0.0\n",
      "FPR(2.0) = 0.0\n",
      "F1-Score(0.0) = 0.9777777777777777\n",
      "F1-Score(1.0) = 1.0\n",
      "F1-Score(2.0) = 0.9655172413793104\n",
      "Weighted precision:  0.9816053511705686\n",
      "Weighted recall:  0.9807692307692307\n",
      "Weighted F1 score:  0.9806513409961686\n",
      "Weighted false positive rate:  0.014102564102564101\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val predictionAndLabels = testData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = dtModel.predict(features)\n",
    "  (prediction, label)\n",
    "}\n",
    "'''\n",
    "predictions = dtModel.predict(testData.map(lambda point: point.features))\n",
    "predictionAndLabels = predictions.zip(testData.map(lambda lp: lp.label))\n",
    "\n",
    "'''\n",
    "// Instantiate metrics object\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "'''\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "'''\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)\n",
    "'''\n",
    "print(\"Confusion matrix:\")\n",
    "#print(metrics.confusionMatrix())\n",
    "\n",
    "'''\n",
    "// Overall Statistics\n",
    "val precision = metrics.precision\n",
    "val recall = metrics.recall // same as true positive rate\n",
    "val f1Score = metrics.fMeasure\n",
    "println(\"Summary Statistics\")\n",
    "println(s\"Precision = $precision\")\n",
    "println(s\"Recall = $recall\")\n",
    "println(s\"F1 Score = $f1Score\")\n",
    "'''\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall() # same as true positive rate\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Statistics\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 Score = \", f1Score)\n",
    "\n",
    "'''\n",
    "// Precision by label\n",
    "val labels = metrics.labels\n",
    "labels.foreach { l =>\n",
    "    println(s\"Precision($l) = \" + metrics.precision(l))\n",
    "}\n",
    "'''\n",
    "labels = trainingData.map(lambda l:l.label).distinct().sortBy(lambda l: l).collect()\n",
    "for l in labels:\n",
    "    print(\"Precision(%s) = %s\" % (l,metrics.precision(l)))\n",
    "    \n",
    "'''\n",
    "// Recall by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"Recall($l) = \" + metrics.recall(l))\n",
    "}\n",
    "'''\n",
    "for l in labels:\n",
    "    print(\"Recall(%s) = %s\" % (l,metrics.recall(l)))\n",
    "\n",
    "'''\n",
    "// False positive rate by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n",
    "}\n",
    "'''\n",
    "for l in labels:\n",
    "    print(\"FPR(%s) = %s\" % (l,metrics.falsePositiveRate(l)))\n",
    "\n",
    "'''\n",
    "// F-measure by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n",
    "}\n",
    "'''\n",
    "for l in labels:\n",
    "    print(\"F1-Score(%s) = %s\" % (l,metrics.fMeasure(l)))\n",
    "\n",
    "'''\n",
    "// Weighted stats\n",
    "println(s\"Weighted precision: ${metrics.weightedPrecision}\")\n",
    "println(s\"Weighted recall: ${metrics.weightedRecall}\")\n",
    "println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n",
    "println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")\n",
    "'''\n",
    "print(\"Weighted precision: \", metrics.weightedPrecision)\n",
    "print(\"Weighted recall: \", metrics.weightedRecall)\n",
    "print(\"Weighted F1 score: \", metrics.weightedFMeasure())\n",
    "print(\"Weighted false positive rate: \", metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#label-based-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Complete evaluation on test set for Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Summary Statistics\n",
      "Precision =  0.5769230769230769\n",
      "Recall =  0.5769230769230769\n",
      "F1 Score =  0.5769230769230769\n",
      "Precision(0.0) = 0.0\n",
      "Precision(1.0) = 1.0\n",
      "Precision(2.0) = 0.40540540540540543\n",
      "Recall(0.0) = 0.0\n",
      "Recall(1.0) = 1.0\n",
      "Recall(2.0) = 1.0\n",
      "FPR(0.0) = 0.0\n",
      "FPR(1.0) = 0.0\n",
      "FPR(2.0) = 0.5945945945945946\n",
      "F1-Score(0.0) = 0.0\n",
      "F1-Score(1.0) = 1.0\n",
      "F1-Score(2.0) = 0.5769230769230769\n",
      "Weighted precision:  0.4054054054054054\n",
      "Weighted recall:  0.5769230769230769\n",
      "Weighted F1 score:  0.4548816568047337\n",
      "Weighted false positive rate:  0.17151767151767153\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val predictionAndLabels = testData.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = nbModel.predict(features)\n",
    "  (prediction, label)\n",
    "}\n",
    "'''\n",
    "predictions = nbModel.predict(testData.map(lambda point: point.features)).map(lambda pre: float(pre))\n",
    "predictionAndLabels = predictions.zip(testData.map(lambda lp: lp.label))\n",
    "\n",
    "'''\n",
    "// Instantiate metrics object\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "'''\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "'''\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)\n",
    "'''\n",
    "print(\"Confusion matrix:\")\n",
    "#print(metrics.confusionMatrix())\n",
    "\n",
    "'''\n",
    "// Overall Statistics\n",
    "val precision = metrics.precision\n",
    "val recall = metrics.recall // same as true positive rate\n",
    "val f1Score = metrics.fMeasure\n",
    "println(\"Summary Statistics\")\n",
    "println(s\"Precision = $precision\")\n",
    "println(s\"Recall = $recall\")\n",
    "println(s\"F1 Score = $f1Score\")\n",
    "'''\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall() # same as true positive rate\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Statistics\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 Score = \", f1Score)\n",
    "\n",
    "'''\n",
    "// Precision by label\n",
    "val labels = metrics.labels\n",
    "labels.foreach { l =>\n",
    "    println(s\"Precision($l) = \" + metrics.precision(l))\n",
    "}\n",
    "'''\n",
    "labels = trainingData.map(lambda l:l.label).distinct().sortBy(lambda l: l).collect()\n",
    "for l in labels:\n",
    "    print(\"Precision(%s) = %s\" % (l,metrics.precision(l)))\n",
    "    \n",
    "'''\n",
    "// Recall by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"Recall($l) = \" + metrics.recall(l))\n",
    "}\n",
    "'''\n",
    "for l in labels:\n",
    "    print(\"Recall(%s) = %s\" % (l,metrics.recall(l)))\n",
    "\n",
    "'''\n",
    "// False positive rate by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n",
    "}\n",
    "'''\n",
    "for l in labels:\n",
    "    print(\"FPR(%s) = %s\" % (l,metrics.falsePositiveRate(l)))\n",
    "\n",
    "'''\n",
    "// F-measure by label\n",
    "labels.foreach { l =>\n",
    "    println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n",
    "}\n",
    "'''\n",
    "for l in labels:\n",
    "    print(\"F1-Score(%s) = %s\" % (l,metrics.fMeasure(l)))\n",
    "\n",
    "'''\n",
    "// Weighted stats\n",
    "println(s\"Weighted precision: ${metrics.weightedPrecision}\")\n",
    "println(s\"Weighted recall: ${metrics.weightedRecall}\")\n",
    "println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n",
    "println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")\n",
    "'''\n",
    "print(\"Weighted precision: \", metrics.weightedPrecision)\n",
    "print(\"Weighted recall: \", metrics.weightedRecall)\n",
    "print(\"Weighted F1 score: \", metrics.weightedFMeasure())\n",
    "print(\"Weighted false positive rate: \", metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
